{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# superclass of neural network \"modules\" (layers)\n",
    "class Module:\n",
    "    \"\"\"\n",
    "    Module is a super class. It could be a single layer, or a multilayer perceptron.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train = True\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        z = f(a); a is the input, and h is the output.\n",
    "        \n",
    "        Inputs:\n",
    "        _input: a\n",
    "        \n",
    "        Returns:\n",
    "        output z\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Compute:\n",
    "        gradient w.r.t. _input\n",
    "        gradient w.r.t. trainable parameters\n",
    "        \n",
    "        Inputs (in lecture notation):\n",
    "        _input: a \n",
    "        _gradOutput: dL/dz\n",
    "        \n",
    "        Returns:\n",
    "        gradInput: dL/dz\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return the value of trainable parameters and its corresponding gradient (Used for grandient descent)\n",
    "        \n",
    "        Returns:\n",
    "        params, gradParams\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn the module into training mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn the module into evaluate mode.(Only useful for Dropout layer)\n",
    "        Ignore it if you are not using Dropout.\n",
    "        \"\"\"\n",
    "        self.train = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a class representing a sequence of modules (a layered network)\n",
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Sequential provides a way to plug layers together in a feed-forward manner.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        self.layers = [] # layers contain all the layers in order\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer) # Add another layer at the end\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.layers) # How many layers.\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        Feed forward through all the layers, and return the output of the last layer\n",
    "        \"\"\"\n",
    "        # self._inputs saves the input of each layer\n",
    "        # self._inputs[i] is the input of i-th layer\n",
    "        self._inputs = [_input]\n",
    "        for i in range(self.size()):\n",
    "            # The output of (i-1)-th layer as the _input of i-th layer\n",
    "            self._inputs.append(self.layers[i].forward(self._inputs[i]))\n",
    "        # The last element of self._inputs is the output of last layer\n",
    "        self._output = self._inputs[-1]\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        Backpropogate through all the layers using chain rule.\n",
    "        \"\"\"\n",
    "        # self._gradInputs[i] is the gradient of loss w.r.t. the input of i-th layer\n",
    "        self._gradInputs = [None] * (self.size() + 1)\n",
    "        self._gradInputs[self.size()] = _gradOutput\n",
    "        for i in reversed(range(self.size())):\n",
    "            self._gradInputs[i] = \\\n",
    "                self.layers[i].backward(self._inputs[i], self._gradInputs[i + 1])\n",
    "        self._gradInput = self._gradInputs[0]\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return trainable parameters and its corresponding gradient in a nested list\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        gradParams = []\n",
    "        for m in self.layers:\n",
    "            _p, _g = m.parameters()\n",
    "            if _p is not None:\n",
    "                params.append(_p)\n",
    "                gradParams.append(_g)\n",
    "        return params, gradParams\n",
    "\n",
    "    def training(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into training mode\n",
    "        \"\"\"\n",
    "        Module.training(self)\n",
    "        for m in self.layers:\n",
    "            m.training()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Turn all the layers into evaluate mode\n",
    "        \"\"\"\n",
    "        Module.evaluate(self)\n",
    "        for m in self.layers:\n",
    "            m.evaluate()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected(Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer (parameters include a matrix of weights a vector of biases)\n",
    "    \"\"\"\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        Module.__init__(self)\n",
    "        # Initalization\n",
    "        stdv = 2./inputSize\n",
    "        self.weight = np.random.normal(0, stdv, (inputSize, outputSize))\n",
    "        self.bias = np.random.normal(0, stdv, outputSize)\n",
    "        \n",
    "        # stdv = 1./np.sqrt(inputSize)\n",
    "        # self.weight = np.random.uniform(-stdv, stdv, (inputSize, outputSize))\n",
    "        # self.bias = np.random.uniform(-stdv, stdv, outputSize)\n",
    "        \n",
    "        self.gradWeight = np.ndarray((inputSize, outputSize))\n",
    "        self.gradBias = np.ndarray(outputSize)\n",
    "        \n",
    "        \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = W * input + b\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output = self._input.dot(self.weight) + self.bias\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradWeight = gradOutput * input\n",
    "        gradBias = \n",
    "        gradInput =  gradWeight * gradOutput\n",
    "        \"\"\"\n",
    "        self.gradWeight.fill(0)\n",
    "        self.gradBias.fill(0)\n",
    "        \n",
    "        self._gradInput = _gradOutput.dot(self.weight.T)\n",
    "        self.gradWeight += _input.T.dot(_gradOutput)\n",
    "        \n",
    "        self.gradBias += _gradOutput.mean(axis = 0)\n",
    "        \n",
    "        \n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return weight and bias and their g\n",
    "        \"\"\"\n",
    "        return [self.weight, self.bias], [self.gradWeight, self.gradBias]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"\n",
    "    ReLU activation, not trainable.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output = np.clip(self._input, 0, None)\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \"\"\"\n",
    "        self._gradInput = _gradOutput * (self._input > 0)\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    \"\"\"\n",
    "    sigmoid activation, not trainable.\n",
    "    HC: this is an element-wise sigmoid. Note the backward gradient\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        Module.__init__(self)\n",
    "        return\n",
    "    \n",
    "    def forward(self, _input):\n",
    "        \"\"\"\n",
    "        output = max(0, input)\n",
    "        \"\"\"\n",
    "        self._input = _input\n",
    "        self._output = 1. /  (1 + np.exp(-self._input))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        \"\"\"\n",
    "        gradInput = gradOutput * mask\n",
    "        mask = _input > 0\n",
    "        \"\"\"\n",
    "        self._gradInput = _gradOutput * (1. - self._output) * self._output\n",
    "        return self._gradInput\n",
    "        \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parametersm, return None\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "class Dropout(Module):\n",
    "    \"\"\"\n",
    "    A dropout layer\n",
    "    \"\"\"\n",
    "    def __init__(self, p = 0.5):\n",
    "        Module.__init__(self)\n",
    "        self.p = p #self.p is the drop rate, if self.p is 0, then it's a identity layer\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        self._output = _input\n",
    "        if self.p > 0:\n",
    "            if self.train:\n",
    "                # Randomize a mask from bernoulli distrubition\n",
    "                self.mask = np.random.binomial(1, 1 - self.p, _input.shape).astype('float64')\n",
    "                # Scale the mask to compensate for the signal weakening. Remember\n",
    "                self.mask /= 1 - self.p\n",
    "                self._output *= self.mask\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _gradOutput):\n",
    "        self._gradInput = _gradOutput\n",
    "        if self.train:\n",
    "            if self.p > 0:\n",
    "                self._gradInput *= self.mask\n",
    "        return self._gradInput\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        No trainable parameters.\n",
    "        \"\"\"\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftMaxLoss(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def forward(self, _input, _label):\n",
    "        \"\"\"\n",
    "        Softmax and cross entropy loss layer. Should return a scalar, since it's a\n",
    "        loss. (It's almost identical to what we had in Pset 2)\n",
    "\n",
    "        Inputs:\n",
    "        _input: N x C\n",
    "        _labels: N x C, one-hot\n",
    "\n",
    "        Returns: loss (scalar)\n",
    "        \"\"\"\n",
    "        self._input = _input - _input.max(1)[:, np.newaxis]\n",
    "        self._logprob = self._input - np.log(np.exp(self._input).sum(1)[:, np.newaxis])\n",
    "        \n",
    "        self._output = np.mean(np.sum(-self._logprob * _label, 1))\n",
    "        return self._output\n",
    "    \n",
    "    def backward(self, _input, _label):\n",
    "        self._gradInput = -1 * (_label - np.exp(self._logprob)) / _input.shape[0]\n",
    "        return self._gradInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.39717044345\n",
      "7.27653442711e-09\n"
     ]
    }
   ],
   "source": [
    "# Test softmaxloss, the relative error should be small enough\n",
    "def test_sm():\n",
    "    crit = SoftMaxLoss()\n",
    "    gt = np.zeros((3, 10))\n",
    "    gt[np.arange(3), np.array([1,2,3])] = 1\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(x, gt)\n",
    "\n",
    "    print(crit.forward(x, gt))\n",
    "\n",
    "    gradInput = crit.backward(x, gt)\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    #print(gradInput)\n",
    "    #print(gradInput_num)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "    \n",
    "test_sm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing FullyConnected\n",
      "3.10565369313e-08\n",
      "testing ReLU\n",
      "7.36022825478e-10\n",
      "testing 2-layer model\n",
      "2.74620328116e-08\n"
     ]
    }
   ],
   "source": [
    "# Test modules, all the relative errors should be small enough (on the order of 1e-6 or smaller)\n",
    "def test_module(model):\n",
    "\n",
    "    model.evaluate()\n",
    "\n",
    "    crit = TestCriterion()\n",
    "    gt = np.random.random((3,10))\n",
    "    x = np.random.random((3,10))\n",
    "    def test_f(x):\n",
    "        return crit.forward(model.forward(x), gt)\n",
    "\n",
    "    gradInput = model.backward(x, crit.backward(model.forward(x), gt))\n",
    "    gradInput_num = numeric_gradient(test_f, x, 1, 1e-6)\n",
    "    print(relative_error(gradInput, gradInput_num, 1e-8))\n",
    "\n",
    "# Test fully connected\n",
    "model = FullyConnected(10, 10)\n",
    "print('testing FullyConnected')\n",
    "test_module(model)\n",
    "\n",
    "# Test ReLU\n",
    "model = ReLU()\n",
    "print('testing ReLU')\n",
    "test_module(model)\n",
    "\n",
    "# Test Dropout\n",
    "#model = Dropout()\n",
    "#test_module(model)\n",
    "\n",
    "# Test Sequential\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(10, 10))\n",
    "model.add(ReLU())\n",
    "model.add(FullyConnected(10, 10))\n",
    "#model.add(Dropout())\n",
    "print('testing 2-layer model')\n",
    "test_module(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,5))\n",
    "crit = TestCriterion()\n",
    "test_learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.71992048871\n",
      "1.31619500308\n",
      "1.31132613878\n",
      "0.5763378553\n",
      "0.604524005174\n",
      "0.471512359734\n",
      "0.229554708128\n",
      "0.199302397076\n",
      "0.249691528335\n",
      "0.21167737992\n",
      "0.167189972186\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "params, gradParams = model.parameters()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 1000 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    sgdm(params, gradParams, test_learning_rate, 0.8, state)\n",
    "    if it > 10000:\n",
    "        break\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.469909203261\n",
      "0.399767970199\n",
      "0.349676869202\n",
      "0.307785192283\n",
      "0.27113201816\n",
      "0.21855587882\n",
      "0.22268530393\n",
      "0.195538427321\n",
      "0.174265216864\n",
      "0.133663360399\n",
      "0.145336644844\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "params, gradParams = model.parameters()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 1000 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    sgdmom(params, gradParams, test_learning_rate, 0.8, state)\n",
    "    if it > 10000:\n",
    "        break\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.382865712813\n",
      "0.00066643146758\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n",
      "2.16156274621e-06\n"
     ]
    }
   ],
   "source": [
    "# Test gradient descent, the loss should be lower and lower\n",
    "trainX = np.random.random((10,5))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(FullyConnected(5, 3))\n",
    "model.add(ReLU())\n",
    "model.add(Dropout())\n",
    "model.add(FullyConnected(3, 1))\n",
    "\n",
    "crit = TestCriterion()\n",
    "\n",
    "params, gradParams = model.parameters()\n",
    "\n",
    "it = 0\n",
    "state = None\n",
    "while True:\n",
    "    output = model.forward(trainX)\n",
    "    loss = crit.forward(output, None)\n",
    "    if it % 1000 == 0:\n",
    "        print(loss)\n",
    "    doutput = crit.backward(output, None)\n",
    "    model.backward(trainX, doutput)\n",
    "    # sgdm(params, gradParams, 0.01, 0.8, state)\n",
    "    sgdadam(params, gradParams, lr = test_learning_rate, state = state)\n",
    "    if it > 10000:\n",
    "        break\n",
    "    it += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above code tests Adam, SGDMomentum, and Nesterov Momentum respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start to work on real data. The first one is Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load large trainset.\n",
      "(50000, 784)\n",
      "(50000, 10)\n",
      "Load valset.\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import FMNIST_utils\n",
    "\n",
    "# We only consider large set this time\n",
    "print(\"Load large trainset.\")\n",
    "Xlarge,Ylarge = FMNIST_utils.load_data(\"Tr\")\n",
    "print(Xlarge.shape)\n",
    "print(Ylarge.shape)\n",
    "\n",
    "print(\"Load valset.\")\n",
    "Xval,Yval = FMNIST_utils.load_data(\"Vl\")\n",
    "print(Xval.shape)\n",
    "print(Yval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ylarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X, model):\n",
    "    \"\"\"\n",
    "    Evaluate the soft predictions of the model.\n",
    "    Input:\n",
    "    X : N x d array (no unit terms)\n",
    "    model : a multi-layer perceptron\n",
    "    Output:\n",
    "    yhat : N x C array\n",
    "        yhat[n][:] contains the score over C classes for X[n][:]\n",
    "    \"\"\"\n",
    "    return model.forward(X)\n",
    "\n",
    "def error_rate(X, Y, model):\n",
    "    \"\"\"\n",
    "    Compute error rate (between 0 and 1) for the model\n",
    "    \"\"\"\n",
    "    model.evaluate()\n",
    "    res = 1 - (model.forward(X).argmax(-1) == Y.argmax(-1)).mean()\n",
    "    model.training()\n",
    "    return res\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "def runTrainVal(X,Y,model,Xval,Yval,trainopt):\n",
    "    \"\"\"\n",
    "    Run the train + evaluation on a given train/val partition\n",
    "    trainopt: various (hyper)parameters of the training procedure\n",
    "    During training, choose the model with the lowest validation error. (early stopping)\n",
    "    Assumes (global) variable crit containing the loss (training \"criterion\" to be minimized)\n",
    "    \"\"\"\n",
    "    \n",
    "    params, gradParams = model.parameters()\n",
    "    \n",
    "    eta = trainopt['eta']\n",
    "    \n",
    "    N = X.shape[0] # number of data points in X\n",
    "    \n",
    "    # Save the model with lowest validation error\n",
    "    minValError = np.inf\n",
    "    saved_model = None # Save the best model accoring to validation error\n",
    "    \n",
    "    shuffled_idx = np.random.permutation(N)\n",
    "    start_idx = 0\n",
    "    for iteration in range(trainopt['maxiter']):\n",
    "        if iteration % int(trainopt['eta_frac'] * trainopt['maxiter']) == 0:\n",
    "            eta *= trainopt['etadrop']\n",
    "        # form the next mini-batch\n",
    "        stop_idx = min(start_idx + trainopt['batch_size'], N)\n",
    "        batch_idx = range(N)[int(start_idx):int(stop_idx)]\n",
    "        \n",
    "        s_idx = shuffled_idx[batch_idx]\n",
    "        \n",
    "        bX = X[s_idx,:]\n",
    "        bY = Y[s_idx,:]\n",
    "\n",
    "        score = model.forward(bX)\n",
    "        loss = crit.forward(score, bY)\n",
    "        # note: this computes loss on the *batch* only, not on the entire training set!\n",
    "        \n",
    "        dscore = crit.backward(score, bY)\n",
    "        model.backward(bX, dscore)\n",
    "        \n",
    "        # Update the data using preferred update rule\n",
    "        \n",
    "        if trainopt['update'] == 'sgdm':\n",
    "            sgdm(params, gradParams, eta, weight_decay = trainopt['lambda'])    \n",
    "        elif trainopt['update'] == 'sgd':\n",
    "            sgd(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "        elif trainopt['update'] == 'nesterov':\n",
    "            sgdmom(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "        elif trainopt['update'] == 'adam':\n",
    "            sgdadam(params, gradParams, eta, weight_decay = trainopt['lambda'])\n",
    "        else:\n",
    "            print(\"the optimizer {} cannot be identified\".format(trainopt['update']))\n",
    "            break\n",
    "\n",
    "\n",
    "        start_idx = stop_idx % N\n",
    "        \n",
    "        if (iteration % trainopt['display_iter']) == 0:\n",
    "            #compute train and val error; multiply by 100 for readability (make it percentage points)\n",
    "            trainError = 100 * error_rate(X, Y, model)\n",
    "            valError = 100 * error_rate(Xval, Yval, model)\n",
    "            print('{:8} batch loss: {:.3f} train error: {:.3f} val error: {:.3f}'.format(iteration, loss, trainError, valError))\n",
    "            \n",
    "            # early stopping: save the best model snapshot so far (i.e., model with lowest val error)\n",
    "            if valError < minValError:\n",
    "                saved_model = deepcopy(model)\n",
    "                minValError = valError\n",
    "        \n",
    "    return saved_model, minValError, trainError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(input_size, hidden_size, output_size, activation_func = 'ReLU', dropout = 0):\n",
    "    \"\"\"\n",
    "    Build a model:\n",
    "    input_size: the dimension of input data\n",
    "    hidden_size: the dimension of hidden vector, hidden_size == 0 means only one layer;\n",
    "        hidden_size = [h1, h2, ...] specifies multiple layers of sizes h1, h2 etc.\n",
    "    output_size: the output size of final layer (typically, number of classes).\n",
    "    activation_func: ReLU, sigmoid (defined above), Tanh (you'd have to define), etc. \n",
    "    dropout: the dropout rate: if dropout == 0, this is equivalent to no dropout\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    if type(hidden_size) is int:\n",
    "        hidden_size = [hidden_size] # ensure it's a list\n",
    "    \n",
    "    prev_size=input_size\n",
    "    \n",
    "    # add hidden layer(s) as requested\n",
    "    if hidden_size[0] == 0: # no hidden layer\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for l in range(len(hidden_size)):\n",
    "            model.add(FullyConnected(prev_size, hidden_size[l]))\n",
    "            prev_size=hidden_size[l]\n",
    "            \n",
    "            if activation_func == 'ReLU':\n",
    "                model.add(ReLU())\n",
    "            elif activation_func == 'sigmoid':\n",
    "                model.add(Sigmoid())\n",
    "                \n",
    "            if dropout > 0:\n",
    "                model.add(Dropout(dropout))\n",
    "                \n",
    "    # now add output layer  \n",
    "    model.add(FullyConnected(prev_size, output_size))\n",
    "\n",
    "        \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changes made: either system change, or paramter change. \n",
    "\n",
    "System Change:\n",
    "1. Initialization strategy: since I stick to ReLU, switch to Kaiming's N(0, 2/InputSize)\n",
    "2. Add Adam Optimizer to manage SGD. Learning rate management may now be removed.\n",
    "\n",
    "Paramter Change:\n",
    "1. Try different dropout rate.\n",
    "2. Try size of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgdadam(x, dx, lr, beta1 = 0.9, beta2 = 0.999, state = None, weight_decay = 0):\n",
    "    # sgd adam optimizer. \n",
    "    \"\"\" see http://cs231n.github.io/neural-networks-3/\n",
    "    \n",
    "    m = beta1*m + (1-beta1)*dx\n",
    "    v = beta2*v + (1-beta2)*(dx**2)\n",
    "    x += - learning_rate * m / (np.sqrt(v) + eps)\n",
    "    \"\"\"\n",
    "    if not state:\n",
    "        if type(x) is list:\n",
    "            state = [None] * len(x)\n",
    "        else:\n",
    "            state = {}\n",
    "            state['m'] = np.zeros(x.shape)\n",
    "            state['v'] = np.zeros(x.shape)\n",
    "    if type(x) is list:\n",
    "        for _x, _dx, _state in zip(x, dx, state):\n",
    "            sgdadam(_x, _dx, lr, beta1, beta2, _state, weight_decay)\n",
    "    else:\n",
    "        state['m'] = beta1 * state['m'] + (1 - beta1) * dx\n",
    "        state['v'] = beta2 * state['v'] + (1 - beta2) * (dx ** 2)\n",
    "        x += - lr * state['m'] / (np.sqrt(state['v']) + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Common Setup shared by experiments\n",
    "\n",
    "NFEATURES = Xlarge.shape[1]\n",
    "# we will maintain a record of models trained for different values of lambda\n",
    "# these will be indexed directly by lambda value itself\n",
    "trained_models = dict()\n",
    "trainopt = {\n",
    "    'eta': 1e-3,   # initial learning rate\n",
    "    'maxiter': 60000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 5000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 128,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # drop eta after every eta_frac*maxiter\n",
    "    'update': 'sgdm' # SGD with momentum (using the default momentum value, see utils.py)\n",
    "}\n",
    "lambda_=0.0001\n",
    "\n",
    "trainopt['lambda'] = lambda_\n",
    "crit = SoftMaxLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.307 train error: 89.538 val error: 89.330\n",
      "    5000 batch loss: 0.339 train error: 14.106 val error: 15.260\n",
      "   10000 batch loss: 0.274 train error: 11.566 val error: 13.200\n",
      "   15000 batch loss: 0.381 train error: 9.810 val error: 11.950\n",
      "   20000 batch loss: 0.380 train error: 9.164 val error: 11.480\n",
      "   25000 batch loss: 0.273 train error: 9.518 val error: 12.340\n",
      "   30000 batch loss: 0.246 train error: 8.106 val error: 11.490\n",
      "   35000 batch loss: 0.174 train error: 7.948 val error: 11.080\n",
      "   40000 batch loss: 0.234 train error: 7.594 val error: 11.020\n",
      "   45000 batch loss: 0.205 train error: 7.360 val error: 10.900\n",
      "   50000 batch loss: 0.151 train error: 6.950 val error: 10.830\n",
      "   55000 batch loss: 0.172 train error: 6.854 val error: 10.920\n",
      "train set model [ h = 200 200  ], lambda= 0.0001 ] --> train error: 6.85, val error: 10.83\n"
     ]
    }
   ],
   "source": [
    "# do not use lambda as key. Use an experiment id\n",
    "arch_index = 0 # modifications: Kaiming init strategy. \n",
    "\n",
    "hidden_size_=[200,200]\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.309 train error: 80.028 val error: 80.180\n",
      "    5000 batch loss: 0.380 train error: 14.734 val error: 15.820\n",
      "   10000 batch loss: 0.516 train error: 12.674 val error: 13.730\n",
      "   15000 batch loss: 0.350 train error: 11.340 val error: 12.730\n",
      "   20000 batch loss: 0.264 train error: 10.492 val error: 12.270\n",
      "   25000 batch loss: 0.359 train error: 10.064 val error: 12.090\n",
      "   30000 batch loss: 0.375 train error: 9.868 val error: 12.020\n",
      "   35000 batch loss: 0.367 train error: 9.544 val error: 11.760\n",
      "   40000 batch loss: 0.350 train error: 9.196 val error: 11.350\n",
      "   45000 batch loss: 0.253 train error: 9.016 val error: 11.360\n",
      "   50000 batch loss: 0.268 train error: 8.968 val error: 11.390\n",
      "   55000 batch loss: 0.227 train error: 8.858 val error: 11.290\n",
      "train set model [ h = 200 200  ], lambda= 0.0001 ] --> train error: 8.86, val error: 11.29\n"
     ]
    }
   ],
   "source": [
    "# do not use lambda as key. Use an experiment id\n",
    "arch_index = 1 # modifications: Kaiming init strategy. 50% dropout\n",
    "\n",
    "hidden_size_=[200,200]\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice: dropout alone, as regularization, does not improve the perf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.303 train error: 95.282 val error: 95.320\n",
      "    5000 batch loss: 0.572 train error: 19.354 val error: 19.820\n",
      "   10000 batch loss: 0.434 train error: 15.220 val error: 16.460\n",
      "   15000 batch loss: 0.298 train error: 13.158 val error: 14.320\n",
      "   20000 batch loss: 0.360 train error: 12.680 val error: 13.900\n",
      "   25000 batch loss: 0.392 train error: 11.922 val error: 13.300\n",
      "   30000 batch loss: 0.315 train error: 11.408 val error: 12.900\n",
      "   35000 batch loss: 0.290 train error: 11.340 val error: 13.110\n",
      "   40000 batch loss: 0.371 train error: 10.722 val error: 12.410\n",
      "   45000 batch loss: 0.316 train error: 10.784 val error: 12.410\n",
      "   50000 batch loss: 0.262 train error: 10.384 val error: 12.030\n",
      "   55000 batch loss: 0.196 train error: 10.304 val error: 12.390\n",
      "train set model [ h = 200 200 200  ], lambda= 0.0001 ] --> train error: 10.30, val error: 12.03\n"
     ]
    }
   ],
   "source": [
    "# do not use lambda as key. Use an experiment id\n",
    "arch_index = 3 # modifications: Kaiming init strategy. hidden size [200 200 200] .nestorov momentum\n",
    "\n",
    "trainopt['update'] = 'nesterov'\n",
    "trainopt['etadrop'] = 0.5\n",
    "hidden_size_=[200,200,200]\n",
    "\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curiously, going deeper doesn't really help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.557 train error: 81.876 val error: 81.540\n",
      "    5000 batch loss: 0.346 train error: 10.242 val error: 12.170\n",
      "   10000 batch loss: 0.221 train error: 10.024 val error: 13.140\n",
      "   15000 batch loss: 0.129 train error: 6.504 val error: 10.730\n",
      "   20000 batch loss: 0.113 train error: 5.760 val error: 10.870\n",
      "   25000 batch loss: 0.236 train error: 5.074 val error: 10.620\n",
      "   30000 batch loss: 0.184 train error: 5.042 val error: 11.340\n",
      "   35000 batch loss: 0.090 train error: 4.262 val error: 10.720\n",
      "   40000 batch loss: 0.071 train error: 3.332 val error: 10.480\n",
      "   45000 batch loss: 0.075 train error: 3.100 val error: 10.730\n",
      "   50000 batch loss: 0.072 train error: 2.682 val error: 10.350\n",
      "   55000 batch loss: 0.077 train error: 2.734 val error: 10.530\n",
      "train set model [ h = 200  ], lambda= 0.0001 ] --> train error: 2.73, val error: 10.35\n"
     ]
    }
   ],
   "source": [
    "# do not use lambda as key. Use an experiment id\n",
    "arch_index = 4 # modifications: Kaiming init strategy. hidden size [200] nestorov momentum\n",
    "\n",
    "trainopt['update'] = 'sgdm'\n",
    "trainopt['etadrop'] = 0.5\n",
    "hidden_size_=[200]\n",
    "\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple 200 hidden layer can be better than [200, 200]. Interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.880 train error: 76.154 val error: 76.070\n",
      "    5000 batch loss: 0.370 train error: 10.952 val error: 12.570\n",
      "   10000 batch loss: 0.348 train error: 9.488 val error: 11.510\n",
      "   15000 batch loss: 0.189 train error: 8.354 val error: 11.050\n",
      "   20000 batch loss: 0.291 train error: 7.336 val error: 10.910\n",
      "   25000 batch loss: 0.358 train error: 6.804 val error: 11.000\n",
      "   30000 batch loss: 0.298 train error: 6.392 val error: 10.840\n",
      "   35000 batch loss: 0.322 train error: 5.960 val error: 10.510\n",
      "   40000 batch loss: 0.264 train error: 5.630 val error: 10.430\n",
      "   45000 batch loss: 0.231 train error: 5.706 val error: 10.440\n",
      "   50000 batch loss: 0.181 train error: 5.260 val error: 10.540\n",
      "   55000 batch loss: 0.186 train error: 5.044 val error: 10.510\n",
      "train set model [ h = 200  ], lambda= 0.0001 ] --> train error: 5.04, val error: 10.43\n"
     ]
    }
   ],
   "source": [
    "arch_index = 4 # modifications: Kaiming init strategy. dropout 50%, hidden size [200] nestorov momentum\n",
    "\n",
    "trainopt['update'] = 'sgdm'\n",
    "trainopt['etadrop'] = 0.5\n",
    "hidden_size_=[200]\n",
    "\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout alone doesn't seem to help with over-fitting. Need stronger regularization from lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.266 train error: 77.602 val error: 77.690\n",
      "    5000 batch loss: 0.309 train error: 9.638 val error: 11.790\n",
      "   10000 batch loss: 0.232 train error: 8.954 val error: 12.270\n",
      "   15000 batch loss: 0.199 train error: 6.128 val error: 10.670\n",
      "   20000 batch loss: 0.134 train error: 5.036 val error: 10.670\n",
      "   25000 batch loss: 0.074 train error: 4.800 val error: 10.760\n",
      "   30000 batch loss: 0.044 train error: 4.264 val error: 10.900\n",
      "   35000 batch loss: 0.058 train error: 2.904 val error: 10.470\n",
      "   40000 batch loss: 0.034 train error: 2.764 val error: 10.220\n",
      "   45000 batch loss: 0.069 train error: 2.318 val error: 10.100\n",
      "   50000 batch loss: 0.094 train error: 2.292 val error: 10.240\n",
      "   55000 batch loss: 0.045 train error: 1.522 val error: 10.050\n",
      "train set model [ h = 400  ], lambda= 0.0001 ] --> train error: 1.52, val error: 10.05\n"
     ]
    }
   ],
   "source": [
    "arch_index = 5 # modifications: Kaiming init strategy. dropout 50%, hidden size [200] nestorov momentum\n",
    "\n",
    "trainopt['update'] = 'sgdm'\n",
    "trainopt['etadrop'] = 0.5\n",
    "hidden_size_=[400]\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.585 train error: 81.908 val error: 81.540\n",
      "    5000 batch loss: 0.400 train error: 10.524 val error: 12.200\n",
      "   10000 batch loss: 0.347 train error: 8.766 val error: 11.010\n",
      "   15000 batch loss: 0.212 train error: 7.480 val error: 10.720\n",
      "   20000 batch loss: 0.305 train error: 6.364 val error: 10.220\n",
      "   25000 batch loss: 0.206 train error: 6.050 val error: 10.160\n",
      "   30000 batch loss: 0.203 train error: 5.526 val error: 10.190\n",
      "   35000 batch loss: 0.212 train error: 4.842 val error: 9.830\n",
      "   40000 batch loss: 0.196 train error: 4.598 val error: 9.850\n",
      "   45000 batch loss: 0.148 train error: 4.404 val error: 10.070\n",
      "   50000 batch loss: 0.187 train error: 4.066 val error: 9.760\n",
      "   55000 batch loss: 0.187 train error: 3.912 val error: 9.670\n",
      "train set model [ h = 400  ], lambda= 0.0001 ] --> train error: 3.91, val error: 9.67\n"
     ]
    }
   ],
   "source": [
    "arch_index = 5 # modifications: Kaiming init strategy. dropout 50%, hidden size [200] nestorov momentum\n",
    "\n",
    "trainopt['update'] = 'sgdm'\n",
    "trainopt['etadrop'] = 0.5\n",
    "trainopt['lambda'] = 1e-3\n",
    "hidden_size_=[400]\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5)\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(Xlarge, Ylarge, model, Xval, Yval, trainopt)\n",
    "trained_models[arch_index] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 2.806 train error: 66.105 val error: 66.520\n",
      "    5000 batch loss: 0.318 train error: 10.877 val error: 11.230\n",
      "   10000 batch loss: 0.298 train error: 9.052 val error: 9.300\n",
      "   15000 batch loss: 0.406 train error: 8.177 val error: 8.510\n",
      "   20000 batch loss: 0.227 train error: 6.783 val error: 6.690\n",
      "   25000 batch loss: 0.225 train error: 6.343 val error: 6.370\n",
      "   30000 batch loss: 0.227 train error: 5.940 val error: 5.670\n",
      "   35000 batch loss: 0.213 train error: 5.577 val error: 5.490\n",
      "   40000 batch loss: 0.113 train error: 5.417 val error: 5.450\n",
      "   45000 batch loss: 0.146 train error: 4.988 val error: 4.950\n",
      "   50000 batch loss: 0.163 train error: 4.645 val error: 4.630\n",
      "   55000 batch loss: 0.253 train error: 4.648 val error: 4.730\n",
      "train set model [ h = 400  ], lambda= 0.0001 ] --> train error: 4.65, val error: 4.63\n"
     ]
    }
   ],
   "source": [
    "# combine test and val data to train a final network for submission based on best setup\n",
    "# note: since train and val are combined here, val error is irrelevant\n",
    "# now: [400], dropout 50%, lambda 1e-3, sgdm\n",
    "\n",
    "Xcomb = np.concatenate((Xlarge, Xval), axis = 0)\n",
    "Ycomb = np.concatenate((Ylarge, Yval), axis = 0)\n",
    "trainopt['update'] = 'sgdm'\n",
    "trainopt['etadrop'] = 0.5\n",
    "trainopt['lambda'] = 1e-3\n",
    "hidden_size_=[400]\n",
    "model = build_model(NFEATURES, hidden_size_, 10, dropout = 0.5)\n",
    "# -- model trained on large train set\n",
    "best_trained_model, valErr, trainErr = runTrainVal(Xcomb, Ycomb, model, Xval, Yval, trainopt)\n",
    "\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### visualize the learnt features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAD5CAYAAAADWbBJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXnYjeX7xX1KKRWJCJF5yDwkUz0RmclM5nkmRFFknkWJ\nZCwylilTMhMyz2PmIYQyhlK8f9/nWt+f53ju7XmP9z3W579rHadn731f93DZe53rivPgwQMTQggh\nhBAx47H/t9+AEEIIIcT/l9FiSgghhBAiBFpMCSGEEEKEQIspIYQQQogQaDElhBBCCBECLaaEEEII\nIUKgxZQQQgghRAi0mBJCCCGECIEWU0IIIYQQIXg8Nl9s8ODBELd+8eJFqHvsMVzjZciQITBOnjw5\n1MydOxe0EiVKgFa5cmXQli9fDtoLL7wQGD/+OB6uLFmygHbs2DHQ2L/1f9/MbMGCBaCdPn06ML5z\n5w7UxI8fH7T69euDVqhQoTggxoBz587BXA4cOBDq3nzzTdBu3LgRGCdMmBBqNm/eDNobb7wB2h9/\n/AFatmzZQLt//35gfPnyZah56qmnQGPH+ubNm6A1btwYtFq1aoH277//BsZdunSBmgkTJoBWp04d\n0EqUKBGRuTQzq169Oszn7du3oe6TTz4BbdCgQYFxrly5oGbx4sWgpUqVCrQnn3wStIIFC4K2e/fu\nwJhd54kTJwZt+/btoD3xxBOg5c+fH7R//vkHtGvXrgXGvXv3hprvv/8etBUrVoDWo0ePiMznyJEj\nYS7ZcU2dOjVo2bNnD4zZvOXIkQO0S5cugcau4VKlSoHWq1evwLhTp07R+lvsvEiTJg1ojOnTp4Pm\nr8WJEydCjb9+zcyqVq0KWuHChSN2bdaoUQPmc8CAAVDHnn9ly5YNjNnn7tChA2hNmjQBjV0n06ZN\nA23y5MmBsb/fm/F7+ZEjR0Bj59rWrVtBK1myJGj+mc6e+y+99BJoCRIkAM3MHjqf+mZKCCGEECIE\nWkwJIYQQQoRAiykhhBBCiBDEqmcqc+bMoLHf8tlvm82aNQuMq1SpAjVDhgwBrWXLlqAx70P16tVB\n868RN25cqJk6dSpo8+bNA435SNKlSwca8+34Y/Tiiy9Czd69e0EbMWIEaN999x1oMeH48eOgZcyY\nEbS+ffuC5ufp1VdfhZq8efOCNnv2bNBee+010Fq1agXa+++/Hxjny5cPal5++WXQTp06Bdqff/4J\n2g8//ABa8eLFQfNerQ0bNkAN80Mw/08keffdd0Fjn5N5iQ4fPhwYM59ct27dQGMejIYNG4LGPnvF\nihUDY+Z53LJlC2jMT1epUiXQzp8/D1q8ePFAmz9/fmDM7kt///03aP6YRRJ2bRYpUgQ05tvyn/G/\n//6DmjFjxoD27LPPgta6dWvQvHfRDK+dv/76C2qYf4n5UDdu3AhazZo1Qbt37x5oP/74Y2DMzvW1\na9eCxnxIhQsXBi2mxImDdp3PP/8cNOZv836uokWLQg17/0xj/uDOnTuD5p917Nxr2rQpaMwDd+LE\nCdAqVKgAWtu2bUHzz+ZNmzZBDfOFRkVFgea9hAx9MyWEEEIIEQItpoQQQgghQqDFlBBCCCFECLSY\nEkIIIYQIQZwHDyAP7JExdepUeLF69epBHQuuPHPmTGDMghNZANf+/ftBY+bD3Llzg1auXLmHvq+U\nKVOCxkyWzMjJwhmZKXT48OGBcYECBaCGmXlv3boF2ttvvx2RMLkffvgB5pI1GAwePBg0b+ZjplFm\nxGfGw+eeew409rkLFSoUGK9cuRJqduzYARozIw4bNgw0FqqZM2dO0JIkSRIYs7BDFjz4xRdfgDZ+\n/PiIBQPOmzcP5vPXX3+FOnbuecMwa3JgAZ0szI8ZWllI39KlSwNjdqxTpEgBGgsGTJQoEWjLli0D\njZ3Ld+/eDYyZebpr166gffvtt6DlyZMnIvO5aNEimEtmgmfND97Yz+6pS5YsAY01ErGw0oULF4Lm\nDdbe1G/Gzx8WoMmO66hRo0Bj5+jMmTMD4549e0INC1qeMWMGaEOHDo3Ytcmem6wxZt++faD5Bh12\nLb333nugnT17FjTWgMFCcP2/ZfdQdh74JjMz3gjCGktq164Nmg/UZU1C7H7AjPazZs1SaKcQQggh\nxKNEiykhhBBCiBBoMSWEEEIIEQItpoQQQgghQhCrCegnT54EjZnJ7ty5A5o3DDPDrzcQmpk1b94c\ntKNHj4LG0lfHjRsXGLOd6ZlZjZnq2Q7tFy5cAG3gwIGglS5dOjBm5snHHsN1cbt27UCLFMzY7824\nZmhoNcNEXJZ2/vTTT4O2e/du0FhqOUtU98bmNm3aQA1LxmfzxpJ0t23bBtq5c+dA8zCjc7JkyUBj\nu6dHEmb6bdSoEWjMcOrTkv1O9WbcMMtM45kyZQKNNSP4dG32vvLkyQPa2LFjQatVqxZozJTL7kve\n1PrHH39ATZMmTUBjZvZZs2aBFhNY8v6gQYNAY6nO/p43Z84cqGHp2/HjxweNHVdmbPYNACyVmqWp\ns+uQPU++/vpr0FijwPr16wNj1gzUvn170KJznYeBGf4TJEgAGjvfveH8448/hprnn38eNLaLBzuv\n2Hnsm8OmTJkCNR07dgSNpZ2z5iR2rn311Veg+dR8du716tULNLZbQ3TQN1NCCCGEECHQYkoIIYQQ\nIgRaTAkhhBBChCBWQzubNm0KL8Z29Ga/13ofSdy4caGG+S3YDvYsjJDtOu+PDfPUVK9eHbRDhw6B\nxgIn2U7opUqVAs0Hfg4dOhRqmL/l8uXLoDVr1iwiYXJ169aFufQ7lJuZvfjii6D5oFM2l2x39mLF\nioH2zTffgPb222+D5gMi2Q7rLFyRBbAyrwbbwT46v+2z8z9hwoSgMV9ZunTpIhYMuHv3bphPdk1c\nv34dtCtXrgTGLIyThS6y4D52PNiO8OPHjw+MmSdr586doOXLlw805tFhgZDMg9ijR4/AuHHjxlCT\nLVs20JhnZMuWLRGZzxYtWsBcMt9Z3bp1QfNBmyzgmHkSWSAi89gw0qdPHxizQN0OHTqAxsJtmTeM\n+TZ/++030Lwnjt0/mT+KnbPZs2eP2LUZFRUF88kCLtm5ffz48cCYhatevHgRtL59+0br77Nj5P8t\nmxMWxsmuE/Zvmd+NPS+8H5V5mVnY9YABA0DbuXOnQjuFEEIIIR4lWkwJIYQQQoRAiykhhBBCiBBo\nMSWEEEIIEYJYNaC/88478GJffPEF1LEgw1y5cgXGiRMnhpqDBw+CxszCbDd5v7u2mdmIESMC406d\nOkHNpUuXQGPBfSx8rFq1aqAxo2irVq0C4/79+0PNjh07QGMmvxkzZkTEGHnmzBmYSxZ2xnYM9w0A\nzz77LNSw8zJt2rSgPffcc6D5EEAzNKrXqFEDao4dOwYaC8tkoZosxPTMmTOg1a9fPzD2O5ubmWXN\nmhU0dk1UqlQpYibXVatWwQFnjSDJkycHbe7cuYFxv379oIaZXJcuXQoaM5yyc8Ebvdm1xMIImfG7\ncOHCoFWpUgW0ypUrg+YN2ix014cYmvFzfvz48RGZz8WLF8MBY2Z/1jCyZ8+ewJgZ+9k9L2PGjKD5\nxhkzs0KFCoHmTdGnT5+GmqRJk4LGTOQsCJk1wbAmCd+wwO5dzPzMmhUGDBgQsWtz6NChMJ+ssYp9\n9mnTpgXGSZIkgRoWiMrCLNm8+AYMM5yrq1evQg0zrrNQ7MOHD4PGmgB844QZPifZ3E2ePBm0MWPG\ngPbcc8/JgC6EEEII8SjRYkoIIYQQIgRaTAkhhBBChECLKSGEEEKIEGBs8yOE7cTOzNr79u0DzZt+\n27RpAzUNGjQAjZlQf//9d9C8Uc8MzcLMgM7SgNOlSwcaSwNevXo1aCz92r9G0aJFocabgP/Xe4sU\nCxYsAK1hw4agfffdd6A9/fTTgXGlSpWgZubMmaD9+eefoB04cAA0dqy9kZw1JsSLFw80Znzdvn07\naCxVn+0wP3bs2MCYGXyZSdSnM5vx4xZTmjdvDhozpjJzvDfuM3P1ihUrQMuQIQNoLHmZzYvfUYDt\nJtCyZUvQypUrB9orr7wCGrs2V61aBVrZsmUD44IFC0INS2JnJu5IwYzI7Hpix9WnvPfu3RtqUqdO\nDVqWLFlAO3r0KGjMOO3N8azJgTUcJUiQADT2mXwKuBm/Nv0OF99++y3UMJM0++yRhN3Ht2zZAppv\nHjDDZpzatWtDDZtj1ijAju3LL78Mmp/Pnj17Qs0HH3wAGksjZ+b1rVu3gpYzZ07QcufOHRiz3SzY\nc+yZZ54BLTromykhhBBCiBBoMSWEEEIIEQItpoQQQgghQqDFlBBCCCFECGLVgD5lyhTQvOHRzOzu\n3bugDRgwIDBmps4WLVqAxszIzEDMXtNTvXp10JgplRntWRq5NzyacQOrT/plSeDMUJk9e3bQIkWp\nUqVAu379OmjsmP3999+Bcdu2baGGJUmzxgT2uZm5N1WqVIHxW2+9BTXz588HjSV3s2TnzJkzgzZ9\n+nTQypcvHxgPHz4caoYOHQoa++yR5MSJE6CxOWap6Mw07nn//fdBY6Z01lDgG0HMMHWavVdmVC1d\nujRobNcBZlZdt24daL4pgqWkz5kzBzTWRBIpMmXKBFqtWrVA840gjBdeeAG0Ro0agbZt2zbQmKF4\n2LBhoKVPnz4wfv3116FmzZo1oDHzMzPHX7lyBbS4ceOC5l+X3Z9ZGv/mzZtBY80bMYXtkrBo0SLQ\n+vTpA5q/V7FrtXv37qDt3bsXNN9sYcaPkW+gYWbzlClTgubv0WZ8Vwp27Tz+OC5lfJI5axRgO2gs\nWbIEtHfeeQc0j76ZEkIIIYQIgRZTQgghhBAh0GJKCCGEECIEWkwJIYQQQoQgDjPUPSpOnjwJL9ak\nSROo8ynRZmg2nDp1KtQwM5xPSjYzu337Nmh//fUXaHHixAmM169fDzXMHNuxY0fQDh8+DBozirI0\nWp8UzdLODx06BBozRdevXz8OiDHg5s2bMJfMhMrMfHny5AmMfbq9GTd5szTckiVLgvbpp5+C5g3L\ndevWhRqWfOvN/2Y84ZulJbN0f5/cf+vWLah58cUXQWOp/WXLlo3IXJqZFS1aFObz7NmzUDd69GjQ\nfvnll8CYpTOz5OsCBQqAxq5DZlb16dfvvvsu1FStWhU0lmDduXNn0MaPHw8am0+fLJ4tWzaoqVev\nHmiscSVS12azZs1gLrt06QJ1zHjszcPMsM8S49mOFBkzZgSNXev+3GY7TbDdLfx9xIwbv1lzBTv+\nderUCYxZI8uNGzdAS5EiBWgVK1aM2LVZuHBhmE+Wis6upw0bNgTGLCmcNW6xhhdmemf3fD/H7J7B\n5oTtiMCea+wamzVrFmjLli0LjNnxWbp0KWj79+8H7fLlyw+dT30zJYQQQggRAi2mhBBCCCFCoMWU\nEEIIIUQItJgSQgghhAhBrBrQt2zZAi92/PhxqNuxYwdoPoE0a9asULNgwQLQmGm5SJEioB09ehS0\nJ5988qE1LCGYJbQyc7xP/jUzq1KlCmg//vhjYMxM+yxJnpmn69SpExFj5KxZs2Au2bGuWLEiaP5Y\nlClTBmpYGrlPmzbjafY+Yd3M7MiRI4ExS8xm+CYEM7ODBw+CxtJ706VLB9qIESMC4+bNm0MNS/dn\npvdu3bpFzOR6+vRpmE+W3M0M4j4tvFmzZlDDUu6ZQZwlHr/99tugffPNNw99X8yky5oMWHMIa4Bg\nzSG+AYWdG8zgmzx5ctCaNWsWkfmcMWNGtK7NadOmgeaN6mxXA5YyzszgzOSdK1cu0LwZmd3/mXmY\n7VrB7hGskYIl1XsjOftbGzduBI0Zp996662IXZubN2+G+fRNH2ZmNWrUAM3fg5ixnDVMNWzYEDS2\nVmjatClo/lndrVs3qFm+fDlo7NqMFy9etP4ta7Dw1zDbjYOluv+POhnQhRBCCCEeJVpMCSGEEEKE\nQIspIYQQQogQxKpnavDgwfBizFfCfqdPnDhxYMx+M2Y7O7OAzrVr14LGdtzu1atXYMx+Hz5//jxo\nLJiOBZ6x9zF58mTQVq9eHRgzT9bEiRNBY0GSp06dishv+QsXLoS5vHnzZrT+rd+pe8KECVDDjjUL\ncKtVqxZo7Fjcu3cvMGYeEhbomDZtWtCY7+Dpp58Gjc25v958GK0ZD5pNlCgRaLly5YqYL2PIkCEw\nn8yn5Y+jGV6v+fPnhxrmGWSeLBbCyo639+Kx3d9fe+010GrXrg1ahw4dQBs3bhxod+7cAc37h1jI\npb+PmJkNHDgQtKioqIjM56RJk2AumfePBSf6YFJ2L2bH66mnngLt888/B42dU37umF8tKioKNOZh\nZYGOLHzz2rVroHnPUf/+/aGGXa/VqlUDLW3atBG7Nps0aQLzmS9fPqhjwabeW8g8rLNnzwbtjTfe\nAI3dM1nAsJ935lljx5bN06lTp0Bjnqa+ffuC5n3VLGA0S5YsoLE1SZMmTeSZEkIIIYR4lGgxJYQQ\nQggRAi2mhBBCCCFCoMWUEEIIIUQIYtWA3qhRI3ixpEmTQl2/fv1A82btdevWQQ0LVPviiy9AW7x4\nMWjMcOeD6FiImw/2NOO7yzOz5+nTp0HzO5ebYZgZC+2sWbMmaMyg2bVr10dmQD9z5gzUMTP1ypUr\nA+M9e/ZADQtWZebn6AYDnjx5MjBmgZEs5JGFwzLzJDPIPvYY/l9l5syZgTEzrjPjNGu46NKlS8RM\nrgcOHID5ZOfn1KlTQfMNEZs3b4aaggULgsbMzQcOHACNNSM8//zzgTELqGXm2AEDBoDGgneZCZWF\nqX711VeB8a1bt6CGzZ0POjUza9iwYUTm8+LFizCXvonFzGzZsmWg+QBddm1mzpwZNGaIZsbmcuXK\ngebN/oULF4aan3/+GbTBgweDxs69CxcugMZM46NGjQqMmVmbNTTNmzcPtIkTJ0bs2vz4449hPuPG\njQt17HN6w/z+/fuhht3P/D3ajJ/HLFTTHzd2fXXv3h001kRSvnx50L7//nvQWAiuD2ZmzQMs9Jlp\ns2fPlgFdCCGEEOJRosWUEEIIIUQItJgSQgghhAiBFlNCCCGEECF4PDZfjBmIO3XqBBozFl6+fDkw\nZjtke5OxGZoKzbjRjdG1a9fAOHfu3FDDdi5nRvhGjRqBljBhQtCOHDkCmjePsrRXZtpnO39HCmZE\nZinLzNjfuHHjwPivv/6Cmk8++QS0Hj16gMZSxlmqbaVKlQJjljT/999/g8YM9EuWLAGNpd63atUK\nNJ+QfeXKFajx5mozbsqNJAcPHgTNm6vNzFKkSAEaM6Z6hg4dClqxYsVAY+f/sWPHQPOmX3Y/YE0f\nrVu3Bi1BggSgsc/ETPQrVqwIjNn52K5dO9D8/SySsAYYZqplZvx///03MGYG4LFjx4LGzpVu3bqB\nxo6rP99Z0wdrJhg0aBBorKGJNXSwa8yfB+x9sF0e6tevD1okYanubJcHdr+ZO3duYMx2CZkxYwZo\n7Nlx7tw50JIkSQLajh07AuMFCxZAjTeHm/HrZPTo0dH6tzlz5gRt5MiRgTFL369bty5orKksOuib\nKSGEEEKIEGgxJYQQQggRAi2mhBBCCCFCoMWUEEIIIUQIYtWAzhKhmTktR44coB0+fDgwZum6zKjn\njcdmZrt37waNpbR6Uxszufr0XjOzvHnzgsaSYplpjhmjfVrvqVOnoIaZ9+bMmQPaW2+9BVpMSJUq\nFWgsVZwlfHsjLzPsM/MqS2MuVKgQaMzQ7o2vzFg+f/580Hr27Ana1q1bQWMmaXYelCxZMjC+du0a\n1LDGhH379oHmk8fDED9+fNBYcwgz/WbMmDEwZkZ+tmPBE088AdqsWbNAY8fDN5GwtHZmPP7ss89A\nq169OmhsPn1DipnZt99+GxizJhUGM4lHCpZ2zszzLDXem/ZZUw9r4Pnvv/9AY/cflrrumytY44C/\n//+v12Q7EbCmCXbP8Snu7H7D5pelokcS9tysV68eaKwJwO+W8cMPP0DNl19+CRprBDl//jxoLJne\nH8c2bdpADXtWN2/eHDT2TEmUKBFozAjvd8Jg523ixIlBY8ejQ4cOoHn0zZQQQgghRAi0mBJCCCGE\nCIEWU0IIIYQQIdBiSgghhBAiBLFqQP/0009BY8nIR48eBc2bVVu2bAk1zMy7cOFC0JhZbdiwYaD5\npNWyZctCTbly5UBjqd/M0Hfp0qVo1T148CAwnjRpEtQ0a9YMNJb2HClq164NGktGZsfMGxSnTJkC\nNdevXweNmQVZHUvv/fjjjwNjZlx/+eWXQVu1ahVozEzNDPklSpQAzZvSWQMG+5wsKT2SeBO5GW+G\nePxxvGXcuXMnMGbGYGY8ZonE7Px/6qmnQPNz3L9/f6hhKdd16tQB7ZlnngGNnQudO3cGzZvt2Tyx\n979t2zbQWrRoAVpMYA02zDzftGlT0LxR/e2334aaP//8EzTWYJMhQwbQWFOGfw1Wwz4TazRJnz49\naN988w1orAHFX8PsOcHM/ex8jyQ+xdyMH2+2c8jw4cMD41u3bkHNoUOHQPPPHDNuJE+ZMiVoDRo0\nCIzZsWYNHr7RysxszJgxoLFGIXbP9J+LNQ+w98EaXqKDvpkSQgghhAiBFlNCCCGEECHQYkoIIYQQ\nIgRaTAkhhBBChCBWDegffPABaMyw2a9fP9C8kdwnWpuZValSBbQ+ffqAxoyvzJzmE4K7d+8ONczc\nyNKYmVGaJfMyE2qePHkCY59qa4ambjNunk6bNi1oMSFLliygsaYAliLszessDZolfvv0cDPedMDS\njX16/fLly6GGGSVbtWoFGkuR37lzJ2jMDOuT3Tt27Ag1X3/9NWgsyT+S/Prrr6AxEyrbKeDq1auB\nMUtmZybXHj16gLZmzRrQWHLxihUrAuN//vkHarJmzQpa5syZQWP/dunSpaCVLl0aNN/IwM4XZoTP\nlCkTaJGCNYckSJAANHav8QZ0dn9mDRjMiMyMwo0bNwbtwoULgTFL6WYp4MyAznZcYA0p7B7hzdrs\nODLDMkvajySsuYWdiywl3ieIs+auu3fvgvbTTz+BVrlyZdAqVqwImt8lYdq0aVDDdklgDTtsnnxj\nmBlvBPPzXqRIEahhDUBPPvkkaNFB30wJIYQQQoRAiykhhBBCiBBoMSWEEEIIEQItpoQQQgghQhCr\nBvRevXqB9u6774K2fft20LyZl6WH+5RrM264ZgZZZuD2xmiWivzmm2+CVr58edDOnDkDGkteZuZL\nn+7KEndZcnSOHDlAixQfffQRaCxVnPH7778Hxrt27YKaYsWKgbZhw4Zo1bG0bW9iZinLLKmazS9L\n2x0yZAho06dPB61evXqB8e3bt6GGnZ8seTyS7NixAzRmLmU7BXgj89SpU6GGNWUwjZm1O3XqBJo3\nSjPzKttdgb1/ZjZn5xBLj/YNECNHjoQaZuZlRv5IwV4vW7ZsoLGdGvx9iiW1v//++6CxpqGaNWuC\nxhoRDh48GBizlHq264PfFeN/vSabc3ZO+WcM25khf/78oLE08kjSt29f0FauXAnakiVLQPNzzOaT\nNQowY7Y36JuZjRgxAjQ/L8xAz5qVmJYsWTLQZs6cCRpL80+TJk1gPH78eKhh9/I4ceKAFh30zZQQ\nQgghRAi0mBJCCCGECIEWU0IIIYQQIYhVzxTbAZoFnjEvxcmTJwPjsWPHQg3b7XnLli2gsd+N2W/E\nPuRr8eLFUPPZZ5+B5n+rNTPLmDEjaCxkrXr16qDNmDEjMGY+HhZsyH5r938rprBwPBZgygJSfQAl\n81pFRUWBdvz4cdB+/vln0FgInQ+gZCGqa9euBS1RokSgeY+HGZ9f5onLnj17YHzgwAGo8X4gs0e/\nM32XLl1A69mzJ2gs/NH7SJg/kJ2zu3fvBo0FzbLd5OvXrx8Ys2vT15iZff7556BdvnwZNObdypAh\nA2g+PJH5xVavXg0aCxiOFCxUloVN3rx5E7QkSZIExuz+yTwl+/fvB42F7LIQUH+/9O/BDMNuzXiQ\nJzv+zOPFPILeh+vDks14CDELfI4kLMCYBWjOmTMHNH/fO3HiBNSw86V58+agsWBi5iP0nlg2T8wb\n3Lt3b9BGjRoFGru/MJ/y5s2bA2P2zGLPGXZfig76ZkoIIYQQIgRaTAkhhBBChECLKSGEEEKIEGgx\nJYQQQggRgjhsZ/hHxa1bt+DFmDGSGXy96ZGZG5kpvUKFCqCNGzcONGbA9Wbtl156CWpYQJ43WJuZ\nFS9eHDRmzGNBcT5MjhlaW7duDVrhwoVBu3PnTswSyRxbt26Fubx69SrUMTPpjRs3AmNmXmUhjClS\npABtxYoVoLEdyb3RmwWaMnN1/PjxQWMmVBaix0LuTp06FRhH9/pjO5mXKVMmInNpZrZixQp4I+zc\n9qGjZmZdu3YNjFkgIgvFZcZyZgZndT4okZlLmcb+PgtiZKG0uXLlAu31118PjHPmzAk1d+7cAY2d\nV1FRURGZz0KFCsFcspBa1uzi772sWadZs2agseaiDz/8ELQ1a9aAljJlysD433//hZqsWbOCNnHi\nRNDYZ8qcOTNobE6WL18eGLP7PbsOR48eDdqyZcsidm2uWrUK5pM1AbBnaerUqQNjdo/24dRmvFGp\nY8eOD/37ZmazZs0KjNl8svfBGrKY0Z7h584M70ss2JqZ+zNlygRar169Hjqf+mZKCCGEECIEWkwJ\nIYQQQoRAiykhhBBCiBBoMSWEEEIIEYJYNaA3adIEXqxx48ZQxwy+PiWb1TDT8i+//AIaS4FlSdc+\nmZclBjPTn09ON+NGPZYwzUzp3mh54cIFqGHme5bM26BBg0gZI2EuWfow21E9Xbp0gTEz7bZq1Qq0\n2rVrg8bMjczc69ONN23aBDUsDZcZFJnJde/evaDdvn0bNL+ze+fOnaGGJfCyVP2qVatGzORasWJF\nmE92LhYtWhQ0f92xXejZ7gTsmDGj8ezZs0HzDR0sCd+nzZuZnT17FjSWnvzuu++Cxoy6/v7CjMEs\n0b5OnTqgHTp0KCLz2bp1a5jLpEmTQh1L+M6SJUtgPGLECKhhc8T+FjP7MzP4rl27AmP2TGAG4/v3\n74PG7veC6PEhAAAgAElEQVTsHlGmTBnQ/D20R48eUMN2dGD3uFSpUj3Sa7Nfv35Qt379etD8s4jt\npMCuL3ZN+HkyM5swYQJovpmLNSe88cYboFWtWhU0dg9ic8Cu640bNwbGrAnD15iZdejQAbQsWbLI\ngC6EEEII8SjRYkoIIYQQIgRaTAkhhBBChECLKSGEEEKIEMSqAV0IIYQQ4v9v6JspIYQQQogQaDEl\nhBBCCBECLaaEEEIIIUKgxZQQQgghRAi0mBJCCCGECIEWU0IIIYQQIdBiSgghhBAiBFpMCSGEEEKE\nQIspIYQQQogQaDElhBBCCBECLaaEEEIIIUKgxZQQQgghRAi0mBJCCCGECIEWU0IIIYQQIdBiSggh\nhBAiBFpMCSGEEEKEQIspIYQQQogQaDElhBBCCBECLaaEEEIIIUKgxZQQQgghRAi0mBJCCCGECIEW\nU0IIIYQQIXg8Nl9s9erVD7x24MABqOvduzdoLVq0CIxz5MgBNWfPngVtwYIFoE2bNg20cePGgZY8\nefLAeNOmTQ+tMTOrUKECaDNmzAAtb968oN24cQO0+/fvB8a///471CRIkAC0p59+GrS+ffvGATEG\n1KlTB+ayUqVKULdkyRLQ+vfvHxhPmDABav7880/QBg8eDFrjxo1Ba9euHWj58+cPjNevXw81SZMm\nBe3UqVOgseOaNm1a0CZNmgRa69atA2M238OHDwetfPnyoDVo0CAic2lmtnbtWpjPn3/+GepKlCgB\n2siRIwPjwoULQw07PhkzZgRt69atoK1cuRK0V155JTBu2bIl1NSvXx+0f//9F7Rr166Bljt3btD6\n9OkDWvXq1QPjVKlSQU3Xrl1BY/ebKVOmRGQ+33vvPZjLfPnyQV2SJElAy549e2C8b98+qNmxYwdo\nBQoUAI3dp5YvXw7aY48F/0/fq1cvqIkbNy5ojz+Ojy92ni1btgw0dn959dVXA+OTJ09Cjb+PmPHz\nbNmyZRG7Ns+fPw/zOWLECKirWrUqaK1atQqMGzZsCDWFChUCjR3vOXPmgMbm099/2bXJnpv37t0D\n7a+//gKNzcF///0H2ltvvRUYFy9eHGrYOTp+/HjQ1q1b99D51DdTQgghhBAh0GJKCCGEECIEWkwJ\nIYQQQoQgVj1TzCdQpUoV0IYMGQLa8ePHA+ONGzdCTa1atUB77rnnQDt06BBoadKkAc37ZWrWrAk1\nKVKkAI15QZifK2HChKB5z4KZ2W+//RYYs9+4T5w4Ea2/HymaNGkC2tWrV0F79tlnQTt//nxg7H0n\nZtzTULp0adC8J8DM7NKlS6B9+umngTH7/Z/9Fs98GZ07dwbt3XffBe37778HbcyYMYExO3+Yl2XW\nrFmgNWjQALSYEicOWgKYD2/dunWgea/GokWLoGbbtm2gMa8J8xwxz0Xfvn0DY3YcM2fODFqzZs1A\nY9429vcGDhwImvdlPPHEE1Czc+dO0GrXrg1apGCePnYes2N95cqVwHjs2LFQkydPHtCYl475EpMl\nSwZaly5dAuOlS5dCTc6cOUHbtWsXaMwj++abb4LGzmP/Gfbu3Qs13qtnZlajRg3QIsnFixdBYz4/\n5herV69eYFyyZEmo6dixI2jMo8meJ1OnTgXtmWeeCYzZ85x5RcuUKQMa8+z556EZ/+xr164NjNes\nWQM17Lp45513QIsO+mZKCCGEECIEWkwJIYQQQoRAiykhhBBCiBBoMSWEEEIIEYJYNaCzkEpmEH/+\n+edB8ya8UqVKQQ0Lxvznn39A279/P2jM8OzDB1mgIAsLY8bLoUOHgsYCw5i57sGDYGYbM5j+8ssv\noDHjbqRg733AgAGgMYPir7/+Ghiz48pMgHPnzgVt8uTJoLHzxxv0P/zwQ6ipWLEiaPPnzwctderU\noH3yySegbdiwAbRbt24FxgcPHoSaY8eOgfbFF1+AFklGjx4NGjMVJ0qUCDQf3Pfxxx9DzdGjR0Fj\n16sPNTXjx9aHarJ5+uCDD0DbsmXLQ//W/4IZtv257M23ZmYpU6YEzTcimJmVLVs2Wu/jYVSrVg00\ndvzZddeoUaPAmBl706VLB1pUVBRo7Hrt168faP4YsmeCD4Y1M6tbty5oPtzZjAeWsnPbB0my5wRr\ngsmQIQNokYQ1tzBT/VdffQWaN2G//vrrUMOuL9ZMxBpvmOYbS5o3bw417Hnlw6nNeEPHnTt3QGPn\ntw/oZQGg7Nj+8ccfoEUHfTMlhBBCCBECLaaEEEIIIUKgxZQQQgghRAi0mBJCCCGECEEcb25+lEyc\nOBFeLFeuXFDHzG/e+MoS0FmqMNslm5kUmTG1UqVKgfGPP/4INYyCBQuCxpLNWVpv/PjxQfPmafaZ\nmJm0WLFioGXKlCkiu5k3atQI5rJbt25Qt2rVKtBmz54dGDOT6+XLl0Fju37nyJEDNJYG75N0feqy\nmdmePXtAY6n9SZMmBY2lQrPmBG8AZbuzs3OAJYi3aNEiYjvTmxnMJ7tOsmbNCpo3ibLjc+DAAdC8\n2dmMpxSznekrV64cGLOmANb0wXZcYCZXZlq+ffs2aL4R4/Tp01DjDdZm3OQ6bdq0iMxnu3btYC43\nb94MdU2bNgWtXLlygTH7zCx5nO364A3AZjzNe8mSJYExM+xXqFAhWn+fJbaz64ndjwsUKBAYs10B\nWCMIa5qIioqK2LXZpUsXmE+Wzs6aQ/w1fO/ePahZuHAhaGzHhffeew80f76Y4c4PK1asgBpmXGdG\n/sOHD4PGUtxHjBgBmn9eFy1aFGp8Q5AZb2yoXLnyQ+dT30wJIYQQQoRAiykhhBBCiBBoMSWEEEII\nEYJYDe1kvpJ27dqBNmzYMNC8N4Z5MNhvs5kyZQKN/d7MvFsTJkwIjDt16gQ1DOZf8r4AM7MsWbKA\nxoLuvKeI7XjOQgDZb/7seMQEFoZ66tQp0I4cOQKa/9wsDPHFF18EjYU8soBU5pn69ttvA2Pmo2A+\nPOatYDu2My9R9+7dQWvbtm1gzH7Hz5YtG2gsVJZ5mmIK88HkzJkTNDYHVatWfejfYseb+WdY+CPz\n7XifEzuHChUqBNorr7wCGgvzW7p0KWjsM3ifDbsHJUmSBLRmzZqBFilee+010NKnTw/anDlzQPPH\nkV2HzAPGjr/3KZrxcEUf5MlCGadNmwbapUuXQGvfvj1oo0aNAo35NH3Ibq1ataCG3Q8edWhnkSJF\nQGvTpg1oPqDTzOynn34KjL0P2Ix7rTZt2gQaO47Mc/Tqq68GxszryjyD33zzDWjs2LKAYeaZ7NGj\nR2DMnvE+qNWMP+e9R5Ohb6aEEEIIIUKgxZQQQgghRAi0mBJCCCGECIEWU0IIIYQQIYhVAzozZrOA\nSxao6MMTWWAbMwYPGjQINGagbNmyJWj16tULjJmhlYWgpUiRAjRmanvppZdA8ztum5klSJAgMM6f\nPz/UsFC7Pn36gBYp4+uFCxdAe/rpp0FjZmr/Hk6ePAk1jz+Op6YPSDTjwaTM2OyNmMws74+zGTct\neqOqGTfWsvBBfzxKlSoFNeyzM7NtJGFzwIzM7H3MmzcvMGYmV3auP/XUU6CxhgK2s/vq1asDYxZ+\nyq6JGzdugPbMM8+AdvPmTdCYadmfa3fv3oUadl0wozoz/MeEdOnSgTZ+/HjQ6tSpA5pvGGGhu1eu\nXAGNHcPChQuDxhoufvnll8CY3VPZsWHGafYMYO+DGcmvX78eGI8cORJqGOy+xM73mMKakthrsiaD\nZMmSBcaTJk2CmjJlyoD23XffgcbCeP0z0sysZs2agTG7j7DrhD33WZMEg50L/hnFzo0OHTqANmDA\ngGi9pkffTAkhhBBChECLKSGEEEKIEGgxJYQQQggRAi2mhBBCCCFCEKsGdGb+rFGjBmhsB/KDBw8G\nxj4R3Yyb0GbOnAkaM0Wz3a/9+9ixYwfUsB23c+fODRozbDNj3qJFi0DzxmiWRJsmTRrQWEpupGDm\n1ccew7U5M+3798XMuMyoyo4X44033gDNzx07x1q1agWaT9E1M8ubNy9oO3fuBG3IkCGg+aYA1kjB\n/t2nn34KWiRh5x27xtiO6t6EytLDmfmTXTvsOmQ7FniNpWYzszzbFeCDDz4Ajf09Zt711yZ7TXYu\nP8rUbJaW37lzZ9B8irwZGtWZGXfKlCmgsSRsnw7/v/7tc889B5qHNSuwuXzhhRdAY+f29u3bQfNJ\n/h999BHU+J0UzMxmzZoFWr58+UCLKcwgzp6ljRo1As3fb9hODeya27x5M2h///03aCwh/7///vs/\nx2a82Wfs2LGglShRAjTWUMAavHzjB9sx4sMPPwSNNUCMGDECNI++mRJCCCGECIEWU0IIIYQQIdBi\nSgghhBAiBFpMCSGEEEKEIFYN6H/99RdozHx4+PBh0LJkyRIYJ0yYEGqmT58OWvHixUFj6dfefGhm\nNmbMmMD4wYMHUMM0ln7LDK0+xdnMbN++faCdOHEiMI5uivPt27dBixTM7P/555+DxpKk//jjj8B4\n+PDhUJM5c2bQmFGdpYWfO3cONH/+MFMkS9FmyeasgSFPnjygtW/fHrTmzZsHxsyoypKAu3fvDtqj\nxqcn/y98kwdLl2bGY9ZQwK4dZr73BllmjN+9e3e0tL59+4LWu3dv0KpUqQLa4MGDA2Of/mxmFhUV\nBRpLdWfp7zGB3VfatWsHWrx48UBr3bp1YMzub2fPngWtdOnSoDHDb6pUqUBr2rRpYMx2qPANSGZm\nZ86cAY2lgDOzPzM2P/vss4HxsWPHoIYljzdu3Bi0SDJhwgTQ+vXrB9q2bdtA889Sdt9j6fKsGYft\n7MGaN/yuEexcZ8/g8+fPg8Yae9j5zRLb/Q4UrKHG34/NeIORDOhCCCGEEI8YLaaEEEIIIUKgxZQQ\nQgghRAi0mBJCCCGECEGsGtDffPNN0Jhpjhms69evHxj/+OOPUMOSf1ma9GeffQYaM717E97y5cuh\nhhmIN27cCFratGlBY8bI1KlTg3bx4sXAmKUNJ02aFLS2bduCxoycMeGrr74CjRmPmcn4t99+C4yZ\nEXbXrl2gMYPuP//8A9rp06dB8+ZJf0zNuLExfvz4oLHPeeXKFdB8A4MZJhKvXLkSapj5nn3OSMJe\nk6V5+1RhMzQps7T//Pnzg8buB8zQygzcPv165MiRUMPMyKwhxTd4mJmtWbMGtP3794PmE/LZPYI1\nggwcOBC0SOEN3Wa8KeP5558HzTd5sN0EWBo5u/+w12TGZm8UZvcttlOGv4+Y8fs422WgSJEioE2d\nOjUwZp+TmaTZ8yqSsBR6lkbO0tn9vfX69etQM2/ePNDu378PGmsMWLZsGWjly5cPjNm9a9WqVaCx\nNHV2DbOmI2ZA97t0sOaTUqVKgcbWB9FB30wJIYQQQoRAiykhhBBCiBBoMSWEEEIIEQItpoQQQggh\nQhCHJdw+Kho0aAAvxkydzBTm02lZovi1a9dAu3fvHmiLFi0CjRktvZGuQIECUHPo0CHQevbsCRpL\nmWVmXlbnE4crV64MNeyze9O+mdmNGzfigBgDkidPDnM5evRoqGOmQm8SZceVJayzlHdmPGTGUZ/K\nO2PGDKjxRlgzntDvz0Uznp7OjJI+LbxTp05QwxKmWXJx3bp1IzKXZmb9+/eH+WQ7FjCOHz8eGD/5\n5JNQU6FCBdAmTpwIGrv2q1WrBppPpmdNH+zc+/DDD0Fjaeevv/46aOy+UaZMmcCYNUmwczlBggSg\nNWjQICLzmTNnTphLdp1s2bIFNG/GZ4ZrlhrNdjFo1qwZaCdPngRt+/btgXHFihWhhqWRs/ub/1tm\n3CD+008/gebPUdYgdPXqVdCYkT9PnjwRuzaHDBkC8+l3dDDjx8in9ntTthnfAaBRo0agseYKZkD3\n7803i5jhDgZm/LnGrp0lS5aAduDAAdASJ0780L/Fmo78c9/MrGPHjg+dT30zJYQQQggRAi2mhBBC\nCCFCoMWUEEIIIUQIYjW00/sLzHjYIfM6+LC9u3fvQs2sWbNAY+FmX3zxBWgvv/zyQ98H80wwf8ur\nr74KGtvVnv3GzcLkjhw5Ehiz8DQWPOh3gI8k9erVA415ZZgnzoeOspA0thP72LFjQWO+HhYGOW7c\nuMCY+aNYsOTs2bNBu3nzJmgsMLZmzZqgeb/JlClToKZdu3aglS1bFjR2TsUU5gNjwX0s9PWPP/4I\njBs0aAA1gwcPBm3YsGGgMY8aC6HMmDFjYMzOPRYeumHDhmi9N3Y9sQBBf+6WKFECaljI6w8//AAa\nO24xoUOHDqD9/vvvoLFA408//TQwZn6gQoUKgcbCVtlxZUGwtWrVCoxLliwJNcxrxbxbVatWBc2f\nn2ZmyZMnB8377rZu3Qo1zEfFnkV58uQBLaYwnxYLd86aNStotWvXDozZMWMh0Mz3+/7774PWsmVL\n0Hw4JjuOn3zyCWjMf8xCRtn9/cyZM6C98847gTGbc3Y+FixYELTooG+mhBBCCCFCoMWUEEIIIUQI\ntJgSQgghhAiBFlNCCCGEECGIVQM627WZmcJYAKU3B7MQwx07doB28eJF0Jhpme06742jzDDOwtNY\nAGjHjh1BYwFqzCDrzc3MUJktWzbQ8ubNC1qkYEZGZjZnZr41a9YExiy81NeYmVWqVAm0J554AjQW\nfJowYcLAePz48VDzyy+/gPbCCy+AxsII06RJAxoLpfQ72LNzZf78+aDNnTsXtEjCTKLsOmSm5VOn\nTgXGly9fhhofmmpm9tVXX4HmTaNmZi1atADNG86TJEkCNXv27AGNBb8ywzY7/5ih3Ru0FyxYADXM\n/M1CQSPFvn37QPNG5P+lPfZY8P/X7BgyQzSb865du4LGjOo+1JE19bz22mugrV27FrTbt2+Dtnfv\nXtDYc8c/F3wwrJlZvnz5QFu4cCFo7JyKKStXrgSNBdm2bdsWNN8EwK7pUaNGgcaem6yOhc9OmzYt\nMG7VqhXUsNDtFClSgMbu7+zaZAZ0H8Y7adIkqGHNbqzZoXDhwqB59M2UEEIIIUQItJgSQgghhAiB\nFlNCCCGEECHQYkoIIYQQIgSxakDftWsXaP379weNGRxXrVoVGLNdylnqLEtYnzlzJmjFihUDzSe2\nf//991Dz888/g5Y5c2bQWII1S9xmRjpvhGcGU28cNePvN6bprh52/FkKODMenz59OjBmqdfM2M92\n/WbG+3nz5oHmDdbMxMh2EGep6ywZnJlE48WLB5o/R7/++muoYY0ODJZEHVMKFCgAGjPyjx49GjSf\nns6OT+nSpUFj6drMGMzmyiexs1R6lkbOdqvPlCkTaOxcS5s2LWhPPfVUYJwuXTqoWbduHWjsHsTS\nu2MCM/xGdycIb75lDRj//PMPaJ06dQKN7ZLAmjf8ecYM+ywdm6VojxkzBjRmtGfNJvXr1w+M48eP\nDzXMQM8aKSIJuwdt374dNPYM8/fkGTNmQA2bk3v37oHGTOPr168HrU+fPoExa27xOxiYmW3atAm0\nHDlyRKuOPS/8c5g1t7DdJtiahDWLefTNlBBCCCFECLSYEkIIIYQIgRZTQgghhBAh0GJKCCGEECIE\nsWpAj4qKAm3FihWgZciQATSfsMsMbMxUmDhxYtCyZs0KWpEiRUBbvHhxYJw6dWqoadq0KWg+0deM\np67/+uuvoDHD3d27dwNjlpDNEpWZKTdSMIMiM36zRGJvvm3fvj3UnDhxAjRmZGSfe86cOaD5Y8bS\nfL3R34wn8DKzOZvLuHHjgpY+ffrA2Cezm2FyrxlPGo4kLEGcNYwwQ7JvamDXyZdffgka++wsYZq9\nN9/YwJpWDh8+DFrRokVBO3/+PGgsbZ8ZU5MmTRoYs8/0448/gsaMzJGC7SjA0sKZ0ds3lrCGmF69\neoHWs2dP0NhuDswk3aRJk8CYnXd169YFjTUOsKTq3Llzg/bKK6+A5q9r9oxhjQmP8j5rZvbxxx+D\nVqFCBdBYcvfjjwcf8WynD/YMS5YsGWjsmmDPNb87AUu09yZ1M34vZ88Gdo2xc943EDCzeffu3UHr\n1q0baNFB30wJIYQQQoRAiykhhBBCiBBoMSWEEEIIEQItpoQQQgghQhDnwYMHsfZigwYNghdjBscG\nDRqA5hNOa9WqBTXM4F6jRg3QWEouM6V7bty4ARozmzMTOTPuHj9+HDRmwL106dJD3xtL4X3ppZdA\nGz58OL7hGDB58mSYS59sbsZNnD7RmpkAWZr6rVu3QBswYABozLA8ffr0wJil5VerVg00ltLNDOjs\n/Nm4cSNoPuHbmzXN+HFk5vjJkydHZC7NzGrVqgXz+d5770Hd3r17QfPG1GvXrkENMyMvWrQINHYu\nsMTt7777LjBm1z5LO2fnI0t6Z8n0adKkAa18+fKB8ZIlS6CGpb9v2bIFtE6dOkVkPsuUKQNzyUzM\nrDlk+fLlgTEzOn/77begsV0f/A4SZmaTJ08GzZvL2bXPjM4+fd6MX09shwXfOGBm1rdv3/9zbMbN\n1MyAXrp06Yhdmxs3boT5ZGn8zLjvnwHs/GTXCWveYM/Nzz//HDS/pmDp4exZumPHDtD87gpmfLeG\n3bt3g+bvQ8ykzkzv7O+nT5/+ofOpb6aEEEIIIUKgxZQQQgghRAi0mBJCCCGECIEWU0IIIYQQIYhV\nA3q3bt3gxeLFiwd1zID78ssvB8YsZfnYsWOgHTp0CLQePXqAxhKa69evHxgzI+x///0HWrp06UBb\nunQpaMyUzkyVXmPHhyX/btu2DbT27dtHxBjZpk0bmMs9e/ZA3aRJk0Dz758l1zMT/8CBA0F77DH8\n/8DChQtB82Z/ZlRlybqffPIJaMzI2KZNG9BWr14NWooUKQJjZvIeN24caOx4ZM+ePWIm13PnzsF8\nsgaJ69evg3blypXA+M6dO1BTuXJl0DZt2gQaMxr71HgzNCkzMzVrbilevDhorMGDpe37e5CZ2Q8/\n/BAYs3lat24daKzZoUuXLhGZz7Jly0bLgM7S/f39mBmu/W4UZnhem/GmCZb87u9TbAeD77//HrSG\nDRuCNnfuXNBYE9LPP/8Mmj8fmYGepa77HSrMzAoVKhSxa7Nu3bown+weNGXKFNB84j97brKGmmXL\nloHGGgPY+/CNJffv34caBmvw8AnuZnxnCWai989v1ph08eJF0MaPHw/a9OnTZUAXQgghhHiUaDEl\nhBBCCBECLaaEEEIIIUKgxZQQQgghRAjQ3fUIYWa1EydOgNahQwfQfOpuwYIFoSZXrlygzZ8/HzRm\nNGZGcp8ou337dqjxBj8zbl5lpvGqVauCljhxYtC88d2naJuZNW3aFLT8+fODFikKFSoEGjMU9+nT\nBzRvHGVG2GHDhoHGDIoVK1YEjc2ThyVtsyRsdqzZucfm97fffgPNn3vMfO9NzWbczM6M6jGlZ8+e\noLFjtH//ftC84ZSdw6wpgxm/hwwZAhpLQfYJ0CVLloQaZkpv3bo1aFevXgWtcePGoLHU5hIlSgTG\nzBw7bdo00CZOnAhapGDXzoIFC0C7cOECaB999FFgzEz39+7dA43tTnDq1CnQqlSpApq/DzLDPjOb\nsxTzDBkygMaStevVqweaT3Fnpv1u3bqBxs7PSMKekezaZEnvfqcAP79mvFGJaawZgV0n/v7ody8x\n48eRmfvZPZk1D7B7jm9EYufG77//DlpMr019MyWEEEIIEQItpoQQQgghQqDFlBBCCCFECGLVM8V2\nk2c7qrOdqF999dXA+OTJk1DDdo5moVz+d2Qzs2+++Qa0qVOnBsaDBg2CGuajmDdvHmjMM8Leb6pU\nqUDzO38zz0LevHlBYz6qSHHw4EHQXnnlFdBGjBgBWvXq1QNjFqLKfCwsGDNlypSgde3aFTTvO2Ne\nNzZvq1atAo0df+bfOHLkCGgHDhz4P9+XGffvPfPMM6BFErazOwuaZR41/97Ytck8O/ny5QON+T7Y\nv/XvLVu2bFDD/JLMg8F8fe3atQOtfPnyoC1fvjwwZp+J+TLY8Y4ULKyUfW4WZun9qs2bN4ca5gFr\n3749aHHiYM6hP//N8H7APDEszJgFeRYrVgw0di9Zu3YtaOfOnQuMO3fuDDUsiLRfv36gRUVFgRZT\nmLeNXSfMU+o9aizcknmImU/r7bffBm39+vWgeT8jCz5mjB07FjTmlUuQIAFoGzZsAK1Vq1aBsb9W\nzXiQ56hRo0D78MMPQfPomykhhBBCiBBoMSWEEEIIEQItpoQQQgghQqDFlBBCCCFECGLVgM5Cs/bs\n2QMaC99MlChRYMzMZOzvexOaGTfNsXC6JUuWBMY7d+6EGhZudvjwYdCYOZCFmLKAueeffz4wZu+/\nZs2aoM2YMQO0N998E7SYkD59etBYyB0zpX/22WeBMTPnszBLdvzLlSsHGgu99Mdiy5YtUMMCKdn8\nMhM222mcBX7648GCN9lnepTNBGbcJFqhQgXQmDkzWbJkgfGaNWug5v333weNhZqywF4W/pgxY8bA\nmAXGXr58GTQ2T8zkyq5XFlTrw1TZ9cXOZfb3mUk8Juzbtw801uxSpEgR0Hxw5YMHD6CGNWCwe16t\nWrVA8+eKGc5J8eLFoYY1qbDgzd69e4PGGle+/PJL0HwQJjuOkydPBs2fi5Fm9OjRoPXt2xc0ZhD3\nYaplypSBml9//RU0FoLLGntYE8DcuXMD46xZs0INO19YM9Hw4cNBmzlzJmhszeCbAFiD0U8//QSa\nX2tEF30zJYQQQggRAi2mhBBCCCFCoMWUEEIIIUQItJgSQgghhAhBrBrQmXGXmcJYAvpbb70VGL/z\nzjtQw5Jup0+fDlqpUqVAW7ZsGWg+idfvhm3G01hZOjhLTz569ChoLJ29UaNGgfGdO3eghiW9P/nk\nk6BFipUrV4LG0oeZcdSbWpmpuX///qB16tQJNJY8zpoJ/HFlRv9q1aqBxs4flvbsU+rN+LnhE+G3\nbt0KNcxMyc4LZtaOKWwHAGZKZ4nYfmcAZiD++uuvQatUqRJorAmApZv7JO1Dhw5Bzf3796P1Plh6\ncp10+K0AAAGWSURBVO7cuUFjid4+bZztYMCMzCz5PlKcPXsWNPZ5mKH+o48+CoxZEnaePHlAY80/\nzDjN/m2BAgUCY2bOT5w4MWjsfhwvXjzQWKOP/5xmZrt27QqMc+TIATXMkO//XaRhc9ClSxfQWOPH\n4sWLA2OWps5M9Wz3A9aowe6FvsmAnXtt27YFjTXsMHO8T1g3480sfl46duwINd6gb8Z3OokO+mZK\nCCGEECIEWkwJIYQQQoRAiykhhBBCiBBoMSWEEEIIEYI4zFAnhBBCCCGih76ZEkIIIYQIgRZTQggh\nhBAh0GJKCCGEECIEWkwJIYQQQoRAiykhhBBCiBBoMSWEEEIIEQItpoQQQgghQqDFlBBCCCFECLSY\nEkIIIYQIgRZTQgghhBAh0GJKCCGEECIEWkwJIYQQQoRAiykhhBBCiBBoMSWEEEIIEQItpoQQQggh\nQqDFlBBCCCFECLSYEkIIIYQIgRZTQgghhBAh0GJKCCGEECIEWkwJIYQQQoRAiykhhBBCiBBoMSWE\nEEIIEQItpoQQQgghQvD/AMC+C5G/cGJBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f50d160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the 10 features\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot\n",
    "\n",
    "final_features = best_trained_model.parameters()[0][-1][0]\n",
    "\n",
    "nrows = 2\n",
    "ncols = 5\n",
    "\n",
    "plt.figure(figsize=(ncols*2, nrows*2))\n",
    "\n",
    "for i in range(nrows*ncols):\n",
    "    plt.subplot(nrows, ncols, i+1)\n",
    "    plt.imshow( final_features[:,i].reshape(20,20), cmap=mpl.cm.Greys)\n",
    "    #plt.imshow(2 - pr_v_10[i].reshape((28,28)), cmap=mpl.cm.Greys)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These features are human-interpretable (if my code is not wrong) since they have been pre-processed by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission-mnist.csv\n"
     ]
    }
   ],
   "source": [
    "#Generate a Kaggle submission file using best_trained_model which you should set based on your experiments\n",
    "#best_model_key = max(trained_models.keys(), key = lambda k: trained_models[k]['val_err'])\n",
    "#best_trained_model = trained_models[best_model_key]['model']\n",
    "kaggleX = FMNIST_utils.load_data('kaggle')\n",
    "kaggleYhat = predict(kaggleX, best_trained_model).argmax(-1)\n",
    "save_submission('submission-mnist.csv', kaggleYhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work on consumer review data (from Pset 3). We will use essentially the same code for the model and training as for Fashion MNIST, but apply it to another data set, with another set of feature functions (mapping text to vectors, instead of mapping images to vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import CR_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine uni-gram and bi-gram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature size:  2049\n",
      "Feature size:  1011\n"
     ]
    }
   ],
   "source": [
    "X_bi, Y_bi, keys_bi = CR_utils.preprocess(use_bigram = True, mincount = 5)\n",
    "X_uni, Y_uni, keys_uni = CR_utils.preprocess(use_bigram = False,mincount = 8)\n",
    "X_train = np.hstack((X_bi['train'], X_uni['train']))\n",
    "Y_train = CR_utils.binarize_labels(np.expand_dims(  Y_uni['train'] , 1))\n",
    "X_val = np.hstack((X_bi['val'], X_uni['val']))\n",
    "Y_val = CR_utils.binarize_labels(np.expand_dims(Y_uni['val'],1))\n",
    "X_test = np.hstack((X_bi['test'], X_uni['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X, Y, keys = CR_utils.preprocess(use_bigram = False,mincount = 8)\n",
    "# X_train = X['train']\n",
    "# Y_train = CR_utils.binarize_labels(np.expand_dims(Y['train'],1))\n",
    "# X_val = X['val']\n",
    "# Y_val = CR_utils.binarize_labels(np.expand_dims(Y['val'],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 0.677 train error: 36.360 val error: 33.000\n",
      "    3000 batch loss: 1.055 train error: 29.982 val error: 30.400\n",
      "    6000 batch loss: 0.528 train error: 27.207 val error: 30.800\n",
      "    9000 batch loss: 0.422 train error: 23.099 val error: 33.200\n",
      "   12000 batch loss: 1.039 train error: 17.369 val error: 30.400\n",
      "   15000 batch loss: 0.000 train error: 11.423 val error: 28.400\n",
      "   18000 batch loss: 0.849 train error: 10.342 val error: 27.600\n",
      "   21000 batch loss: 0.020 train error: 9.586 val error: 26.400\n",
      "   24000 batch loss: 0.014 train error: 9.658 val error: 26.400\n",
      "   27000 batch loss: 0.140 train error: 6.270 val error: 27.800\n",
      "   30000 batch loss: 0.005 train error: 5.586 val error: 27.600\n",
      "   33000 batch loss: 0.006 train error: 3.712 val error: 27.200\n",
      "   36000 batch loss: 0.000 train error: 2.739 val error: 27.600\n",
      "   39000 batch loss: 0.000 train error: 1.946 val error: 27.400\n",
      "   42000 batch loss: 0.249 train error: 1.333 val error: 28.000\n",
      "   45000 batch loss: 0.028 train error: 0.973 val error: 28.400\n",
      "   48000 batch loss: 0.000 train error: 0.793 val error: 28.000\n",
      "train set model [ h = 80 50  ], lambda= 1.0000 ] --> train error: 0.79, val error: 26.40\n"
     ]
    }
   ],
   "source": [
    "# -- training options\n",
    "trainopt = {\n",
    "    'eta': 1e-1,   # initial learning rate\n",
    "    'maxiter': 50000,   # max number of iterations (updates) of SGD\n",
    "    'display_iter': 3000,  # display batch loss every display_iter updates\n",
    "    'batch_size': 1,  \n",
    "    'etadrop': .5, # when dropping eta, multiply it by this number (e.g., .5 means halve it)\n",
    "    'eta_frac': .25,  # how ofter to drop eta\n",
    "    'update': 'sgdm'\n",
    "}\n",
    "\n",
    "NFEATURES = X_train.shape[1]\n",
    "\n",
    "lambda_= 1.\n",
    "hidden_size_=[80, 50]\n",
    "\n",
    "trainopt['lambda'] = lambda_\n",
    "model = build_model(NFEATURES, hidden_size_, 2, dropout = 0.5)\n",
    "crit = SoftMaxLoss()\n",
    "# -- model trained on large train set\n",
    "trained_model,valErr,trainErr = runTrainVal(X_train, Y_train, model, X_val, Y_val, trainopt)\n",
    "trained_models[lambda_] = {'model': trained_model, \"val_err\": valErr, \"train_err\": trainErr }\n",
    "print('train set model [ h = ',end='')\n",
    "for l in range(len(hidden_size_)):\n",
    "    print('%d '%hidden_size_[l],end='')\n",
    "print(' ], lambda= %.4f ] --> train error: %.2f, val error: %.2f' % (lambda_, trainErr, valErr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0 batch loss: 0.699 train error: 64.153 val error: 67.000\n",
      "    3000 batch loss: 1.500 train error: 32.153 val error: 30.000\n",
      "    6000 batch loss: 1.264 train error: 21.771 val error: 24.800\n",
      "    9000 batch loss: 0.460 train error: 29.740 val error: 31.400\n",
      "   12000 batch loss: 1.361 train error: 21.832 val error: 22.200\n",
      "   15000 batch loss: 0.484 train error: 14.412 val error: 17.600\n",
      "   18000 batch loss: 0.174 train error: 13.710 val error: 15.200\n",
      "   21000 batch loss: 0.090 train error: 11.176 val error: 13.000\n",
      "   24000 batch loss: 0.970 train error: 11.481 val error: 13.200\n",
      "   27000 batch loss: 0.463 train error: 8.580 val error: 12.000\n",
      "   30000 batch loss: 1.390 train error: 8.672 val error: 11.600\n",
      "   33000 batch loss: 0.000 train error: 6.198 val error: 7.400\n",
      "   36000 batch loss: 0.602 train error: 5.649 val error: 8.400\n",
      "   39000 batch loss: 0.004 train error: 4.244 val error: 5.200\n",
      "   42000 batch loss: 0.073 train error: 3.664 val error: 4.600\n",
      "   45000 batch loss: 0.002 train error: 3.664 val error: 5.000\n",
      "   48000 batch loss: 0.000 train error: 2.504 val error: 2.800\n"
     ]
    }
   ],
   "source": [
    "# combine the train and val data; note val acc is irrelavant here\n",
    "Xcomb = np.concatenate((X_train, X_val), axis = 0)\n",
    "Ycomb = np.concatenate((Y_train, Y_val), axis = 0)\n",
    "model = build_model(NFEATURES, hidden_size_, 2, dropout = 0.5)\n",
    "best_trained_model, valErr, trainErr = runTrainVal(Xcomb, Ycomb, model, X_val, Y_val, trainopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_key = max(trained_models.keys(), key = lambda k: trained_models[k]['val_err'])\n",
    "# best_trained_model = trained_models[best_model_key]['model']\n",
    "y_hat = predict(X_test, best_trained_model).argmax(-1) * 2 - 1\n",
    "CR_utils.save_submission('submission-CR.csv', y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = np.zeros((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
